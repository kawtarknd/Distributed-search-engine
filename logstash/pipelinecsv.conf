input {
  file {
    path => "C:/Users/utilisateur/Desktop/Distributed-search-engine-main/logstash/data/Major A.I. Projects.csv"
    start_position => "beginning"
    sincedb_path => "NUL"
    codec => plain { charset => "UTF-8" }
  }
}

filter {
  csv {
    separator => ","
    skip_header => "true"
    columns => [
      "Project",
      "Type",
      "Category",
      "Insitution / Company",
      "Developer / Author / Group Lead",
      "Location",
      "Year",
      "Description",
      "Written in",
      "Image",
      "Wikipedia Page",
      "Homepage or Source Code"
    ]
  }

  # Si une ligne vide apparaît (par ex. fin de fichier), on peut la drop
  if ![Project] or [Project] == "" {
    drop { }
  }

  mutate {
    # Renommage pour des clés JSON valides
    rename => {
      "Insitution / Company"            => "institution_company"
      "Developer / Author / Group Lead" => "developer"
      "Written in"                      => "language"
      "Wikipedia Page"                  => "wikipedia_page"
      "Homepage or Source Code"         => "homepage"
    }
    # Conversion de l'année en entier pour pouvoir trier/filtrer
    convert => {
      "Year" => "integer"
    }
  }

  # (Optionnel) Si vous voulez utiliser Year comme timestamp :
  # date {
  #   match => ["Year", "yyyy"]
  #   target => "@timestamp"
  #   remove_field => ["Year"]
  # }
}

output {
  elasticsearch {
    hosts => ["https://localhost:9200"]
    user => "elastic"
    password => "75+D3Nm2SA0rAueloxEU"
    ssl_enabled => true
    ssl_certificate_authorities => ["C:/ELK/elasticsearch-9.0.0/config/certs/http_ca.crt"]
    index => "major_ai_projects1"            # ou "major_ai_projects" si vous préférez un index dédié
    # document_id => "%{Project}"  # décommentez si Project est unique
  }

  stdout {
    codec => rubydebug
  }
}
